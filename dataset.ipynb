{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17d47d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install huggingface-hub\n",
    "#!pip install s2sphere\n",
    "#!pip install pymilvus\n",
    "#!pip install datasets\n",
    "#!pip install transformers \n",
    "#!pip install torch \n",
    "#!pip install earthengine-api \n",
    "#!pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d687833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from pymilvus import MilvusClient, DataType\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import os\n",
    "import s2sphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a10f32f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.Authenticate()\n",
    "ee.Initialize(project=os.getenv('GEE_PROJECT_ID'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e1f16a",
   "metadata": {},
   "source": [
    "Initialize DB (only need to do if not created yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b9eeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient, DataType\n",
    "\n",
    "client = MilvusClient(uri=\"http://localhost:19530\", token=\"root:Milvus\")\n",
    "if \"geoguessr\" not in client.list_databases():\n",
    "    client.create_database(\"geoguessr\")\n",
    "\n",
    "client = MilvusClient(uri=\"http://localhost:19530\", token=\"root:Milvus\", db_name=\"geoguessr\")\n",
    "\n",
    "schema = client.create_schema(auto_id=True, enable_dynamic_field=True)\n",
    "\n",
    "#field names\n",
    "schema.add_field(field_name=\"id\", datatype=DataType.INT64, is_primary=True)\n",
    "schema.add_field(field_name=\"streetclip_vec\", datatype=DataType.FLOAT_VECTOR, dim=768)\n",
    "schema.add_field(field_name=\"alphaearth_vec\", datatype=DataType.FLOAT_VECTOR, dim=64)\n",
    "schema.add_field(field_name=\"gps\", datatype=DataType.JSON)\n",
    "schema.add_field(field_name=\"s2sphere_boundary\", datatype=DataType.VARCHAR, max_length=512)\n",
    "\n",
    "index_params = client.prepare_index_params()\n",
    "\n",
    "index_params.add_index(\n",
    "    field_name=\"streetclip_vec\",\n",
    "    metric_type=\"COSINE\",\n",
    "    index_type=\"HNSW\",\n",
    "    params={\"M\": 16, \"efConstruction\": 500}\n",
    ")\n",
    "\n",
    "index_params.add_index(\n",
    "    field_name=\"alphaearth_vec\",\n",
    "    metric_type=\"L2\",\n",
    "    index_type=\"HNSW\",\n",
    "    params={\"M\": 8, \"efConstruction\": 200}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5399d624",
   "metadata": {},
   "source": [
    "Create Milvus Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fa2c900",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.create_collection(\n",
    "    collection_name=\"world_locations\",\n",
    "    schema=schema,\n",
    "    index_params=index_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9355041",
   "metadata": {},
   "source": [
    "Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9cf48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual_features(image, model, processor):\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.cpu().numpy()[0].tolist()\n",
    "\n",
    "def alphaearth_features(lat, lon):\n",
    "    point = ee.Geometry.Point([lon, lat])\n",
    "\n",
    "    ae_collection = ee.FeatureCollection(\"GOOGLE/SATELLITE_EMBEDDING/V1/ANNUAL\")\n",
    "    ae_image = ae_collection.sort('system:time_start', False).first()\n",
    "\n",
    "    sample = ae_image.sample(point, 10).first().toDictionary().getInfo()\n",
    "    return [sample[f'A{i:02d}'] for i in range(64)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52eeef10",
   "metadata": {},
   "source": [
    "S2sphere token generation (segmenting locations into respective boundaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb2fb9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'osv5m/osv5m' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Dataset scripts are no longer supported, but found osv5m.py",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mosv5m/osv5m\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      3\u001b[0m     split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      4\u001b[0m     streaming\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[1;32m      5\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m first_sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(dataset))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/load.py:1492\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1487\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   1488\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   1489\u001b[0m )\n\u001b[1;32m   1491\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 1492\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m load_dataset_builder(\n\u001b[1;32m   1493\u001b[0m     path\u001b[38;5;241m=\u001b[39mpath,\n\u001b[1;32m   1494\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m   1495\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n\u001b[1;32m   1496\u001b[0m     data_files\u001b[38;5;241m=\u001b[39mdata_files,\n\u001b[1;32m   1497\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m   1498\u001b[0m     features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[1;32m   1499\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[1;32m   1500\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[1;32m   1501\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m   1502\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m   1503\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m   1504\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_kwargs,\n\u001b[1;32m   1505\u001b[0m )\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/load.py:1137\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1136\u001b[0m     features \u001b[38;5;241m=\u001b[39m _fix_for_backward_compatible_features(features)\n\u001b[0;32m-> 1137\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m dataset_module_factory(\n\u001b[1;32m   1138\u001b[0m     path,\n\u001b[1;32m   1139\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m   1140\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[1;32m   1141\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[1;32m   1142\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n\u001b[1;32m   1143\u001b[0m     data_files\u001b[38;5;241m=\u001b[39mdata_files,\n\u001b[1;32m   1144\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m   1145\u001b[0m )\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;66;03m# Get dataset builder class\u001b[39;00m\n\u001b[1;32m   1147\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/load.py:1036\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[0m\n\u001b[1;32m   1031\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[1;32m   1032\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1033\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1034\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1035\u001b[0m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/load.py:994\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    987\u001b[0m     api\u001b[38;5;241m.\u001b[39mhf_hub_download(\n\u001b[1;32m    988\u001b[0m         repo_id\u001b[38;5;241m=\u001b[39mpath,\n\u001b[1;32m    989\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    992\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39mproxies,\n\u001b[1;32m    993\u001b[0m     )\n\u001b[0;32m--> 994\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset scripts are no longer supported, but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n\u001b[1;32m    996\u001b[0m     \u001b[38;5;66;03m# Use the infos from the parquet export except in some cases:\u001b[39;00m\n\u001b[1;32m    997\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_dir \u001b[38;5;129;01mor\u001b[39;00m data_files \u001b[38;5;129;01mor\u001b[39;00m (revision \u001b[38;5;129;01mand\u001b[39;00m revision \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Dataset scripts are no longer supported, but found osv5m.py"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\n",
    "    'osv5m/osv5m', \n",
    "    split='train', \n",
    "    streaming=True, \n",
    "    trust_remote_code=True\n",
    ")\n",
    "first_sample = next(iter(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fe9801",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (181222735.py, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[15], line 19\u001b[0;36m\u001b[0m\n\u001b[0;31m    else:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def s2sphere_boundary(lat, lon):\n",
    "    p = s2sphere.LatLng.from_degrees(lat, lon)\n",
    "    cell = s2sphere.CellId.from_lat_lng(p)\n",
    "    return cell.to_token()\n",
    "\n",
    "image_dict = dict()\n",
    "\n",
    "for sample in stream_ds.take(1000):\n",
    "    lat, lon = sample['lat'], sample['lon']\n",
    "    token = s2sphere_boundary(lat, lon)\n",
    "    if token not in image_dict:\n",
    "       alphaearth_vec = alphaearth_features(lat, lon)\n",
    "       image_dict[token] = {\n",
    "        \"vector\": alphaearth_vec,\n",
    "        \"lat\": lat,\n",
    "        \"lon\": lon,\n",
    "        \"count\": 1\n",
    "    }\n",
    "    else:\n",
    "        image_dict[token][\"count\"] += 1\n",
    "\n",
    "indexed_anchors = []\n",
    "for token, data in image_dict.items():\n",
    "    indexed_anchors.append({\n",
    "        \"vector\": data['vector'],\n",
    "        \"id\": int(token, 16), # Convert hex token to int for Milvus ID\n",
    "        \"lat\": data['lat'],\n",
    "        \"lon\": data['lon']\n",
    "    })\n",
    "\n",
    "\n",
    "client.insert(collection_name=\"world_locations\", data=indexed_anchors)\n",
    "\n",
    "     \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
